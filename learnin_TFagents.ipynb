{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addressed-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noticed-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-happiness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "improved-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ambient-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cubic-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "warming-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aerial-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-wholesale",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-radiation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-citation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ancient-operation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAIAAAD9V4nPAAAGr0lEQVR4nO3dzW2cQBiAYW/kJlIHKSN1QE1QR8oIdaQMcvHF+cEHr5lN3uc5MtLqu6BXM9Kwt+M4ngCg6tPoAQBgJCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQngU+7bs2zJ6Csh5Hj0A5KgdPBQ7QgDShBCuNs3ryar9IlxMCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCGGAaV5PVvdtuWwSQAgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCGGOa15PVfVsumwTihBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCGmeb1ZHXflssmgTIhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCAFIE0IA0oQQgDQhBCBNCGGkaV5PVvdtuWwSyBJCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCANKEEIA0IQQgTQgBSBNCGGya15PVfVsumwSahBCANCEEIE0I4T5u7zDql4EnIQQgTggBSHsePQDw4tuP+ZcnXz9vQyaBFDtCeAi/V/BvD4H7EkIY7yR4WggfTQgBSBNCANKEEIA0IQQgTQhhvJNrEm5QwEcTQngIfwyeCsIFbsdxjJ4B/gfv+bDn9/XVHYkvy9365wWHNwkh3MdjfuHaCw5vcjQKAABQ5WgU7sPRKPyjHI0CkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkCaEAKQJIQBpQghAmhACkObfJwBIsyMEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCANCEEIE0IAUgTQgDShBCAtJ/2DkTxgoGPAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x7FEAF2E6D610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env preview\n",
    "#@test {\"skip\": true}\n",
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-royal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stretch-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-mozambique",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "excess-xerox",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-squad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collected-dress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "# action_spec() returns shape, dtype, name, and min/max values for actions\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-obligation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "atmospheric-cassette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step:\n",
      "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.01061065,  0.00118745, -0.02365191, -0.02122273], dtype=float32))\n",
      "Next time step:\n",
      "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.01063439,  0.19664046, -0.02407636, -0.32127327], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "action = np.array(1, dtype=np.int32)\n",
    "\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "standing-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the enviro again as train and eval environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "gross-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "failing-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the TF enviro wrapper to conver enviro's np arrays to tensors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "alleged-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "virgin-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agent: Crating Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "appreciated-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# it's output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "atomic-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agent: Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adjacent-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pleased-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "raised-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary policy fr eval and deployment\n",
    "eval_policy = agent.policy\n",
    "# another policy, used for collecting (observations?)\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-vacuum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "preceding-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is an example of a policy made outside of an agent.\n",
    "# it performs a random action with each step\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-shepherd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-ultimate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-edwards",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "essential-framing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=())"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need this for the above\n",
    "example_environment = tf_py_environment.TFPyEnvironment(\n",
    "    suite_gym.load('CartPole-v0'))\n",
    "\n",
    "# this returns a policy step, a named tuple with three components:\n",
    "# action (the one to be taken)\n",
    "# state (could be used for stateful policies\n",
    "# info (could be a log)\n",
    "time_step = example_environment.reset()\n",
    "\n",
    "# use to get an action from example policy\n",
    "random_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "parliamentary-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation Metrics\n",
    "# A common metric for evaluating a policy is it's average reward\n",
    "# several episodes are run, yielding an average\n",
    "\n",
    "#@test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        \n",
    "        # runs until out of steps\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "packed-poultry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets's run this on the random policy we made earlier.\n",
    "# This provides a baseline of performance in this enviro\n",
    "# given the hyperperameters\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "timely-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replay Buffer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "seeing-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# holds data collected from enviro (obs?)\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    # specs for the data\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    # HYPERPARAMETER\n",
    "    batch_size=train_env.batch_size,\n",
    "    # HYPERPARAMETER\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bridal-knife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "municipal-selling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "smaller-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "handled-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below is a very common function in RL.\n",
    "# what's going on is recording the data into the replay buffer\n",
    "\n",
    "#@test {\"skip\": true}\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-madrid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-sleep",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-castle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "comfortable-swiss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kenji/.local/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 4), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.float32, action=tf.int64, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "# Ok, so this is creating a pipelie for feeding the buffer to the agent.\n",
    "# we call it dataset, set it up, and then set it to be iterable (next celL)\n",
    "# parralel calls and prefetch are optimizatons but idky\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "muslim-capacity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fe9c075d700>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-alliance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "continent-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training!\n",
    "# in this step we must:\n",
    "#- collect enviro data (obs?)\n",
    "#- use that data to train the agents network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-flush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kenji/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 200: loss = 114.49760437011719\n",
      "step = 400: loss = 41.13872528076172\n",
      "step = 600: loss = 36.06608581542969\n",
      "step = 800: loss = 7.091083526611328\n",
      "step = 1000: loss = 14.981706619262695\n",
      "step = 1000: Average Return = 91.5\n",
      "step = 1200: loss = 5.3616743087768555\n",
      "step = 1400: loss = 19.496503829956055\n",
      "step = 1600: loss = 171.44639587402344\n",
      "step = 1800: loss = 223.86634826660156\n",
      "step = 2000: loss = 54.937442779541016\n",
      "step = 2000: Average Return = 90.30000305175781\n",
      "step = 2200: loss = 270.31982421875\n",
      "step = 2400: loss = 328.6573486328125\n",
      "step = 2600: loss = 402.99639892578125\n",
      "step = 2800: loss = 4387.9365234375\n",
      "step = 3000: loss = 344.230712890625\n",
      "step = 3000: Average Return = 81.9000015258789\n",
      "step = 3200: loss = 215.78903198242188\n",
      "step = 3400: loss = 335.4471435546875\n",
      "step = 3600: loss = 385.788330078125\n",
      "step = 3800: loss = 427.144775390625\n",
      "step = 4000: loss = 270.58428955078125\n",
      "step = 4000: Average Return = 167.3000030517578\n",
      "step = 4200: loss = 353.5269470214844\n",
      "step = 4400: loss = 326.39312744140625\n",
      "step = 4600: loss = 671.4266357421875\n",
      "step = 4800: loss = 7311.404296875\n",
      "step = 5000: loss = 280.68353271484375\n",
      "step = 5000: Average Return = 199.0\n",
      "step = 5200: loss = 625.2395629882812\n",
      "step = 5400: loss = 5415.9150390625\n",
      "step = 5600: loss = 4527.1640625\n",
      "step = 5800: loss = 4588.181640625\n",
      "step = 6000: loss = 14465.234375\n",
      "step = 6000: Average Return = 197.89999389648438\n",
      "step = 6200: loss = 9659.173828125\n",
      "step = 6400: loss = 9976.2734375\n",
      "step = 6600: loss = 5491.14990234375\n",
      "step = 6800: loss = 112361.6796875\n",
      "step = 7000: loss = 3744.146728515625\n",
      "step = 7000: Average Return = 200.0\n",
      "step = 7200: loss = 4933.7412109375\n",
      "step = 7400: loss = 4495.439453125\n",
      "step = 7600: loss = 9250.076171875\n",
      "step = 7800: loss = 8552.177734375\n",
      "step = 8000: loss = 42830.69921875\n",
      "step = 8000: Average Return = 200.0\n",
      "step = 8200: loss = 9565.4130859375\n",
      "step = 8400: loss = 2732.540771484375\n",
      "step = 8600: loss = 6325.8671875\n",
      "step = 8800: loss = 3640.662109375\n",
      "step = 9000: loss = 16729.875\n",
      "step = 9000: Average Return = 200.0\n",
      "step = 9200: loss = 4224.61279296875\n",
      "step = 9400: loss = 6351.8466796875\n",
      "step = 9600: loss = 21207.7890625\n",
      "step = 9800: loss = 18622.447265625\n",
      "step = 10000: loss = 22480.708984375\n",
      "step = 10000: Average Return = 200.0\n",
      "step = 10200: loss = 28631.513671875\n",
      "step = 10400: loss = 71563.96875\n",
      "step = 10600: loss = 41080.609375\n",
      "step = 10800: loss = 2353277.5\n",
      "step = 11000: loss = 81312.640625\n",
      "step = 11000: Average Return = 200.0\n",
      "step = 11200: loss = 162384.734375\n",
      "step = 11400: loss = 182373.234375\n",
      "step = 11600: loss = 6529825.0\n",
      "step = 11800: loss = 125150.0390625\n",
      "step = 12000: loss = 387173.9375\n",
      "step = 12000: Average Return = 200.0\n",
      "step = 12200: loss = 332032.03125\n",
      "step = 12400: loss = 350065.375\n",
      "step = 12600: loss = 722540.0\n",
      "step = 12800: loss = 239640.796875\n",
      "step = 13000: loss = 448277.75\n",
      "step = 13000: Average Return = 200.0\n",
      "step = 13200: loss = 596325.0\n",
      "step = 13400: loss = 600836.125\n",
      "step = 13600: loss = 1082716.375\n",
      "step = 13800: loss = 811096.5\n",
      "step = 14000: loss = 816296.0625\n",
      "step = 14000: Average Return = 200.0\n",
      "step = 14200: loss = 1012482.625\n",
      "step = 14400: loss = 929989.25\n",
      "step = 14600: loss = 1575320.5\n",
      "step = 14800: loss = 1536188.5\n",
      "step = 15000: loss = 1404114.75\n",
      "step = 15000: Average Return = 200.0\n",
      "step = 15200: loss = 68963544.0\n",
      "step = 15400: loss = 3806925.0\n",
      "step = 15600: loss = 2655383.0\n",
      "step = 15800: loss = 2118179.0\n",
      "step = 16000: loss = 2922606.25\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    \n",
    "    \n",
    "# the below periodically prints the loss and average returns    \n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-compression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    # instantiates video (?)\n",
    "    video = open(filename,'rb').read()\n",
    "    # sets the encoder and encodes\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "        <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "        Your browser does not support the video tag.\n",
    "        </video>'''.format(b64.decode())\n",
    "\n",
    "    return IPython.display.HTML(tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-nebraska",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Now iterate through a few episodes of the Cartpole game with the agent.\n",
    "# The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper)\n",
    "# provides a render() method, which outputs an image of the environment state.\n",
    "# These can be collected into a video.\"\n",
    "\n",
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "create_policy_eval_video(random_policy, \"random-agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-carolina",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
